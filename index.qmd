---
title: Empirical Dynamic Modelling
subtitle: Automatic Causal Inference and Forecasting
institute: UNSW Sydney, School of Risk and Actuarial Studies
author: Patrick Laub
date: 7 February 2023
date-format: long
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    slide-number: c/t
    strip-comments: true
    margin: 0.2
    chalkboard:
      boardmarker-width: 5
      grid: false
    include-before: <div class="line right"></div>
    include-after: <script>registerRevealCallbacks();</script>
    code-line-numbers: false
    # footer: Patrick Laub, Stochastic Calculus Seminar, Universit√© de Strasbourg
highlight-style: breeze
jupyter: python3
execute:
  keep-ipynb: true
  echo: true
---

```{python}
#| echo: false
import numpy as np
import numpy.random as rnd
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats
from bs4 import BeautifulSoup
from IPython.display import display, HTML

import matplotlib

import cycler
colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  plt.rcParams['figure.figsize'] = (2.0, 2.0)

def set_rectangular_figures():
  plt.rcParams['figure.figsize'] = (5.0, 2.0)

set_rectangular_figures()

plt.rcParams['figure.dpi'] = 350
plt.rcParams['savefig.bbox'] = "tight"
plt.rcParams['font.family'] = "serif"

plt.rcParams['axes.spines.right'] = False
plt.rcParams['axes.spines.top'] = False

np.set_printoptions(precision=2, threshold=100)

import re

def trim_html_table(html_string, start=3, end=2):
  soup = BeautifulSoup(html_string, 'html.parser')

  # find the row containing "..."
  dots_row = soup.find("td", string="...")

  if dots_row is None:
    return html_string

  target_row = soup.find("td", string="...").parent

  if start:
    # remove all rows after the target row
    for row in list(reversed(target_row.find_previous_siblings("tr")))[start:]:
        row.decompose()

  if end:
    # remove all rows after the target row
    for row in target_row.find_next_siblings("tr")[:-end]:
        row.decompose()

  # the modified html string
  modified_html = str(soup)

  # Find the substring in the format "N rows x M columns" and convert it to "(N, M)"
  regex = re.compile(r"\d+ rows √ó \d+ columns")
  match = regex.search(modified_html)
  if match:
    modified_html = modified_html.replace(match.group(), "") # f"({match.group()})")

  return modified_html


def wrapped_html_repr(fn):
    def wrapped(*args, **kwargs):
        x = fn(*args, **kwargs)
        return trim_html_table(x, end=2)
    return wrapped

pd.DataFrame._repr_html_ = wrapped_html_repr(pd.DataFrame._repr_html_)

# Create a `display_vector` function that calls `trim_html_table` on the output of `_repr_html_`
# but also discards the first column of the generated table.
def display_vector(x, start=None, end=2):
    table = trim_html_table(x._repr_html_(), start=start, end=end)
    
    # Use BeautifulSoup to remove the first column of the table.
    # That is, remove the first <th> inside <thead>, and every <th> inside <tbody>.
    soup = BeautifulSoup(table, 'html.parser')
    soup.find("thead").find("th").decompose()
    for row in soup.find("tbody").find_all("th"):
        row.decompose()
    table = str(soup)
    display(HTML(table))
```

# 

<h2>Goal: automatic causal inference</h2>

<br>

``` r
df <- read.csv("chicago.csv")
head(df)
#>   Time Temperature Crime
#> 1    1       24.08  1605
#> 2    2       19.04  1119
#> 3    3       28.04  1127
#> 4    4       30.02  1154
#> 5    5       35.96  1251
#> 6    6       33.08  1276

library(fastEDM)

crimeCCMCausesTemp <- easy_edm("Crime", "Temperature", data=df)
#> ‚úñ No evidence of CCM causation from Crime to Temperature found.

tempCCMCausesCrime <- easy_edm("Temperature", "Crime", data=df)
#> ‚úî Some evidence of CCM causation from Temperature to Crime found.
```

## {.smaller}

<div style="border: 0px dashed #555"><img src="stata-journal-paper-title.png" /></div>

::: columns
::: column
Jinjing Li<br>
University of Canberra

George Sugihara<br>
University of California San Diego
:::
::: column
Michael J. Zyphur<br>
University of Queensland

Patrick J. Laub<br>
UNSW
:::
:::

<br>

::: callout-note
##  Acknowledgments

Discovery Project DP200100219 and Future Fellowship FT140100629.
:::

## Mirage correlation

::: {.r-stack}
![](mirage-correlations.png)

![](mirage-correlations-sugihara-2012-figure-1.png){.fragment}
:::

::: footer
Source: Sugihara et al. (2012), Detecting causality in complex ecosystems.
:::

## A different view of causality

<br>

Imagine $x_t$, $y_t$, $z_t$ are interesting time series...

_If_ the data is generated according to the nonlinear system:

$$
\begin{aligned}
  x_{t+1}  &= \sigma (y_t - x_t) \\
  y_{t+1}  &= x_t (\rho - z_t) - y_t \\
  z_{t+1}  &= x_t y_t - \beta z_t
\end{aligned}
$$

then $y \Rightarrow x$, both $x, z \Rightarrow y$, and both $x, y \Rightarrow z$.

## Fish brains {.smaller}

<center>
<iframe width="800" height="500" src="https://www.youtube.com/embed/5HtXYKKRA8g?start=1339" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>

Gerald Pao (2021), "How to use Causality without Correlation to Download Brains into Computers", QMNET Seminar.


## Linear/nonlinear dynamical systems {data-visibility="uncounted"}

<br>

Say $\mathbf{x}_t = (x_t, y_t, z_t)$, then if:

::: columns
::: column

$$ \mathbf{x}_{t+1} = \mathbf{A} \mathbf{x}_{t} $$

we have a linear system.

:::
::: column

$$ \mathbf{x}_{t+1} = f(\mathbf{x}_{t}) $$

we have a nonlinear system.

:::
:::

> Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals. (Stanis≈Çaw Ulam)

::: footer
We don't fit a model for $f$, non-parametrically use the data.
Hence the name _empirical_ dynamic modelling. 
:::

## Unobserved variables {data-visibility="uncounted"}

Takens' theorem to the rescue, though...

> Takens' theorem is a deep mathematical result with far-reaching implications.
> Unfortunately, to really understand it, it requires a background in topology.
> (Munch et al. 2020)

::: columns
::: column
![](takens-header.png)
![](takens-contents.png)
:::
::: column
![](takens-simplified.png)
:::
:::

::: footer
Source: Munch et al. (2020), Frequently asked questions about nonlinear dynamics and empirical dynamic modelling, ICES Journal of Marine Science.
:::

# Simplex Algorithm {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Toy problem

```{python}
#| echo: false
rnd.seed(42)

f = 0.137

a = 2 * np.cos(2*np.pi*f)
b = -1

N = 100
t = np.arange(N).astype(float)
x = np.zeros(N)
x[1] = 1.0

for i in range(2, N):
    x[i] = a * x[i-1] + b * x[i-2] + rnd.normal(0, 0.1)

x = x.round(2)
```

::: columns
::: {.column width=75%}

<!-- 
$$
x_t = a x_{t-1} + b x_{t-2} \leftrightarrow x_t = \sin(2 \pi f t)
$$
-->

<br>

```{python}
#| echo: false
plt.rcParams['figure.figsize'] = (5.0, 3.0)
 
# Plot the x vector, adding dots for each point and connecting them with lines
plt.plot(t, x, 'o', color=colors[0])
plt.plot(t, x, '--', color=colors[0], alpha=0.5)
plt.xlabel("$t$")
plt.ylabel("$x$");

set_rectangular_figures()
```

:::
::: {.column width=25%}

```{python}
#| echo: false
df = pd.DataFrame({"\(x_t\)": x})
df.columns.name = "\(t\)"
df
```

:::
:::

<!-- ## Start with time series -->

::: columns
::: {.column width=40%}

```{python}
#| echo: false
# df = pd.DataFrame({"x": x})
# df.columns.name = "t"
# df
```

:::
::: {.column width=40%}

```{python}
#| echo: false
# target = df.copy()
# target.columns.name = "t"
# target.columns = ["next"]
# target["next"] = target["next"].shift(-1) 
# target
```

:::
:::

## Create the embeddings

```{python}
#| echo: false
import fastEDM

E = 3
p = 1
M, y = fastEDM.create_manifold_and_targets(t, x, E=E, tau=1, p=p)

# M = M[:, ::-1]  # reverse the order of the columns

manifold = pd.DataFrame(M)
manifold.columns = ["\(x_t\)"] + ["\(x_{{t-{}}}\)".format(i) for i in range(1, E)]
manifold.columns.name = "\(i\)"

targets = pd.DataFrame(y)
targets.columns = ["\(x_{t+1}\)"]
targets.columns.name = "\(i\)"
```

::: columns
::: {.column width=25%}

```{python}
#| echo: false
display(df)
```

:::
::: {.column width=50%} 

```{python}
#| echo: false
display(manifold) 
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display(targets) 
```

:::
:::

## Making a prediction

```{python}
#| echo: false
point = manifold.iloc[-1, :].values
target = targets.iloc[-1, :].values 

point = [0.57, 0.42, 0.11]
target = -0.25
```

```{python}
#| echo: false
HTML(f"\[ \\boldsymbol{{x}}^* = \\bigl({', '.join(map(str, reversed(point)))} \\bigr) \]")
```

```{python}
#| echo: false
plt.plot(point, 'o', color=colors[0])
plt.plot(point, '--', color=colors[0], alpha=0.5)

plt.axvline(len(point), color=colors[1], ls="--")

# ticks = ["$x_{{t{}}}$".format(i) for i in range(-(E-1), 0)] + ["$x_t$", f"$x_{{t+{p}}}$"]
ticks = ["$t{}$".format(i) for i in range(-(E-1), 0)] + ["$t$", f"$t+{p}$"]

# Set the x tick labels to be the `ticks` variable
plt.xticks(range(len(ticks)), ticks)
plt.ylabel("$x$")
None
```

## Look at the manifold

```{python}
#| echo: false
for i in range(manifold.shape[0]-1): 
  point_i = manifold.iloc[i, :].values
  target_i = targets.iloc[i, 0] 
  plt.plot(point_i, 'o-')
  
  # Plot another line from point_i[-1] to target_i using the same colour as the previous line
  prev_col = plt.gca().lines[-1].get_color()
  
  xs = [len(point_i)-1, len(point_i)]
  ys = [point_i[-1], target_i]

  plt.plot(xs, ys, '--', color=prev_col, alpha=0.5)

plt.axvline(len(point), ls="--", color=colors[1])
plt.xticks(range(len(ticks)), ticks)
plt.ylabel("$x$")
None
```

## Calculate the distances to the point

```{python}
#| echo: false
distances = np.linalg.norm(manifold - point, axis=1)
distances = distances.round(2)
distances = pd.DataFrame({"\(d(\mathbf{x}_i, \mathbf{x}^*)\)": distances})
distances.columns.name = "\(i\)"
``` 

::: columns
::: {.column width=45%} 

```{python}
#| echo: false
display(manifold)
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display_vector(targets)
```

:::
::: {.column width=30%}

```{python}
#| echo: false
display_vector(distances)
```

:::
:::


## Find the nearest neighbours

```{python}
#| echo: false
# Sort the manifold, targets and distances by distance 
order = np.argsort(distances.to_numpy().flatten())
mani_sorted = manifold.iloc[order,:]
targets_sorted = targets.iloc[order]
distances_sorted = distances.iloc[order]
```

::: columns
::: {.column width=45%} 

```{python}
#| echo: false
display(mani_sorted) 
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display_vector(targets_sorted) 
```

:::
::: {.column width=30%}

```{python}
#| echo: false
display_vector(distances_sorted)
```

:::
:::

## Find the nearest neighbours {data-visibility="uncounted"}


```{python}
#| echo: false
# Sort the manifold, targets and distances by distance 
k = len(point) + 1
mani_knn = mani_sorted.head(k)
targets_knn = targets_sorted.head(k)
distances_knn = distances_sorted.head(k)
```


::: columns
::: {.column width=45%} 

```{python}
#| echo: false
display(mani_knn) 
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display_vector(targets_knn) 
```

:::
::: {.column width=30%}

```{python}
#| echo: false
display_vector(distances_knn)
```

:::
:::

## Plot those trajectories

```{python}
#| echo: false
for i in range(mani_knn.shape[0]): 
  point_i = mani_knn.iloc[i, :].values
  target_i = targets_knn.iloc[i, 0] 
  plt.plot(point_i, 'o-')
  
  # Plot another line from point_i[-1] to target_i using the same colour as the previous line
  prev_col = plt.gca().lines[-1].get_color()
  
  xs = [len(point_i)-1, len(point_i)]
  ys = [point_i[-1], target_i]

  plt.plot(xs, ys, '--', color=prev_col, alpha=0.5)

plt.axvline(len(point), ls="--", color=colors[1])
plt.xticks(range(len(ticks)), ticks)
plt.ylabel("$x$")
None
```

## Make a prediction

```{python}
#| echo: false
theta = 1
weights_knn = np.exp(-theta * distances_knn / distances_knn.iloc[0, 0])
weights_knn = weights_knn / weights_knn.sum()
weights_knn = weights_knn.round(2)
weights_knn.columns = ["\(w_i\)"]
weights_knn.columns.name = "\(i\)"
```

::: columns

::: {.column width=25%} 

```{python}
#| echo: false
display(targets_knn) 
```

:::
::: {.column width=25%}

```{python}
#| echo: false
display_vector(distances_knn)
```

:::
::: {.column width=25%}

```{python}
#| echo: false
norm_distances_knn = distances_knn / distances_knn.iloc[0, 0]
norm_distances_knn = norm_distances_knn.round(2)
norm_distances_knn.columns = ["\(\tilde{d}(\mathbf{x}_i, \mathbf{x}^*)\)"]
display_vector(norm_distances_knn)
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display_vector(weights_knn) 
```

:::
:::

<br>
$$w_i = \frac{
  \exp\bigl\{ -\theta \, \tilde{d}(\mathbf{x}_i, \mathbf{x}^*) \bigr\}
  }{
    \sum_{j=1}^k \exp\bigl\{ -\theta \, \tilde{d}(\mathbf{x}_j, \mathbf{x}^*) \bigr\}
  }
$$

## Plot those trajectories (weighted)

```{python}
#| echo: false
for i in range(mani_knn.shape[0]): 
  point_i = mani_knn.iloc[i, :].values
  target_i = targets_knn.iloc[i, 0]
  weight_i = weights_knn.iloc[i, 0]
  plt.plot(point_i, 'o-', alpha=weight_i)
  
  # Plot another line from point_i[-1] to target_i using the same colour as the previous line
  prev_col = plt.gca().lines[-1].get_color()
  
  xs = [len(point_i)-1, len(point_i)]
  ys = [point_i[-1], target_i]

  plt.plot(xs, ys, '--', color=prev_col, alpha=0.75 * weight_i)

plt.axvline(len(point), ls="--", color=colors[1])
plt.xticks(range(len(ticks)), ticks)
plt.ylabel("$x$")
None
```

# S-map Algorithm {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

<br>
<center>

_Sequential Locally Weighted Global Linear Maps_

</center>

## Lorenz system 

Given $x_t$ from the Lorenz system:

```{python}
#| echo: false
df = pd.read_csv("lorenz-rho-50-time-20.csv")
df.columns = ["\(x_t\)", "\(y_t\)", "\(z_t\)"]
df.columns.name = "\(t\)"

df["\(x_t\)"] = df["\(x_t\)"].round(2)
x = df["\(x_t\)"].to_numpy(dtype=float)
x = x.round(2)
t = np.arange(len(x)).astype(float)
```

```{python}
#| echo: false
df["\(x_t\)"].plot(ylabel="$x_t$", xlabel="$t$");
```

predict $x_{t+10}$ given $(x_t, x_{t-20})$.

## Time series to embedding

```{python}
#| echo: false
E = 2
theta = 5.0
tau = 20
p = 10

T = int(np.round(len(t) * 0.9))
M, y = fastEDM.create_manifold_and_targets(t[:T], x[:T], E=E, tau=tau, p=p)

M_pred, y_pred = fastEDM.create_manifold_and_targets(t[T:], x[T:], E=E, tau=tau, p=p)
point = M_pred[-1,:].copy()
true_target = y_pred[-1,0]

manifold = pd.DataFrame(M)
manifold.columns = ["\(x_t\)"] + ["\(x_{{t-{}}}\)".format(i*20) for i in range(1, E)]
manifold.columns.name = "\(i\)"

targets = pd.DataFrame(y)
targets.columns = ["\(x_{t+10}\)"]
targets.columns.name = "\(i\)"
```


::: columns
::: {.column width=25%}

```{python}
#| echo: false
display(df)
```

:::
::: {.column width=50%} 

```{python}
#| echo: false
display(manifold) 
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display(targets) 
```

:::
:::

## Calculate normalised distances

```{python}
#| echo: false
distances = np.linalg.norm(manifold - point, axis=1)
distances = pd.DataFrame({"\(d(\mathbf{x}_i, \mathbf{x}^*)\)": distances})
distances.columns.name = "\(i\)"
norm_distances = distances / distances.mean()
distances = distances.round(2)
norm_distances = norm_distances.round(2)
norm_distances.columns = ["\(\hat{d}(\mathbf{x}_i, \mathbf{x}^*)\)"]

weights = np.exp(-theta * norm_distances)
weights = weights.round(2)
weights.columns = ["\(w_i\)"]
weights.columns.name = "\(i\)"
w = weights.to_numpy().flatten()

# Add a column of ones to the left of the manifold matrix
manifold_with_const = np.hstack([np.ones((manifold.shape[0], 1)), manifold.to_numpy()])

A = w[:,None] * manifold_with_const
B = (w * targets.to_numpy().flatten())
C_t = np.linalg.lstsq(A, B, rcond=None)[0] 
``` 

::: columns
::: {.column width=40%} 

```{python}
#| echo: false
display(manifold) 
```

:::
::: {.column width=20%} 

```{python}
#| echo: false
display_vector(targets) 
```

:::
::: {.column width=20%}

```{python}
#| echo: false
display_vector(distances)
```

:::
::: {.column width=20%}

```{python}
#| echo: false
display_vector(norm_distances)
```

:::
:::

The average of the distances is:
```{python}
#|echo: false
np.round(np.mean(np.array(distances)), 2)
```

## Weights {data-visibility="uncounted"}

::: columns
::: {.column width=40%} 

```{python}
#| echo: false
display(manifold) 
```

:::
::: {.column width=30%} 

```{python}
#| echo: false
display_vector(distances) 
```

:::
::: {.column width=30%}

```{python}
#| echo: false
display_vector(weights)
```

:::
:::

$$w_i = \exp\bigl\{ -\theta \, \hat{d}(\mathbf{x}_i, \mathbf{x}^*) \bigr\}$$

```{python}
#| echo: false
from sklearn import linear_model

X = manifold.to_numpy()
Y = targets.to_numpy().flatten()

ols = linear_model.LinearRegression()
unweighted_model = ols.fit(X, Y)
```

```{python}
#| echo: false
ols = linear_model.LinearRegression()
weighted_model = ols.fit(X, Y, sample_weight=weights.to_numpy().flatten())
```

## Linear auto-regression 

```{python}
#| echo: false
fig, axes = plt.subplots(1, E, figsize=(8, 3))

for i, ax in enumerate(axes):
  ax.scatter(manifold.iloc[:, i], targets, s=0.5)

  ax.set_xlabel("$x_{t}$")
  if i > 0:
    ax.set_xlabel(f"$x_{{t-{tau*i}}}$")

  xs = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)
  preds = np.zeros(len(xs))

  for j, x_j in enumerate(xs):
    avg_point = np.zeros(E)
    avg_point[0:] = manifold.mean(axis=0).values
    avg_point[i] = x_j
    preds[j] = unweighted_model.predict(avg_point[np.newaxis,:])

  ax.plot(xs, preds, color=colors[1], ls="--", alpha=0.5)

  ax.scatter([point[i]], [true_target], marker="x")

axes[0].set_ylabel(f"$x_{{t+{p}}}$")

plt.tight_layout()
None
```

## Linear regression plane

::: columns
::: column

```{python}
#| echo: false
# Create range for each dimension
xx_pred = np.linspace(X[:,0].min(), X[:,0].max(), 30)
yy_pred = np.linspace(X[:,1].min(), X[:,1].max(), 30)
xx_pred, yy_pred = np.meshgrid(xx_pred, yy_pred)
z = Y

model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

predicted = unweighted_model.predict(model_viz)

# Plot model visualization
fig = plt.figure(figsize=(4, 4))
ax = plt.axes(projection='3d')

ax.plot(X[:,0], X[:,1], Y, zorder=15, linestyle='none', marker='.', alpha=0.125)
ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, s=15, facecolor=(0,0,0,0), edgecolor=colors[1])

ax.set_xlabel("$x_{t}$")
ax.set_ylabel("$x_{t-20}$")
ax.set_zlabel("$x_{t+10}$")

ax.view_init(elev=27, azim=-60)

fig.tight_layout()
```

:::
::: column

::: fragment

```{python}
#| echo: false
# Create range for each dimension
xx_pred = np.linspace(X[:,0].min(), X[:,0].max(), 30)
yy_pred = np.linspace(X[:,1].min(), X[:,1].max(), 30)
xx_pred, yy_pred = np.meshgrid(xx_pred, yy_pred)

model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

predicted = weighted_model.predict(model_viz)

# Plot model visualization
fig = plt.figure(figsize=(4, 4))
ax = plt.axes(projection='3d')

ax.plot(X[:,0], X[:,1], Y, zorder=15, linestyle='none', marker='.', alpha=0.125)
ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, s=15, facecolor=(0,0,0,0), edgecolor=colors[1])

ax.set_xlabel("$x_{t}$")
ax.set_ylabel("$x_{t-20}$")
ax.set_zlabel("$x_{t+10}$")

ax.view_init(elev=27, azim=-60)

fig.tight_layout()
```

:::

:::
:::

## Weighted linear regression


```{python}
#| echo: false
fig, axes = plt.subplots(1, E, figsize=(8, 3))

for i, ax in enumerate(axes):
  ax.scatter(manifold.iloc[:, i], targets, alpha=weights, s=0.5)
  ax.set_xlabel("$x_{t}$")
  if i > 0:
    ax.set_xlabel(f"$x_{{t-{tau*i}}}$")

  xs = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)
  preds = np.zeros(len(xs))

  for j, x_j in enumerate(xs):
    avg_point = np.zeros(E)
    avg_point[0:] = (manifold.values * (weights.values / (weights.values.sum()))).sum(axis=0)
    avg_point[i] = x_j
    preds[j] = weighted_model.predict(avg_point[np.newaxis,:])

  ax.plot(xs, preds, color=colors[1], ls="--", alpha=0.5)

  ax.scatter([point[i]], [true_target], marker="x")
  
axes[0].set_ylabel(f"$x_{{t+{p}}}$")

plt.tight_layout()
None
```

# Empirical Dynamic Modelling (EDM) {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Create lagged embeddings

<br>

Given two time series, create $E$-length trajectories

$$ \mathbf{x}_t = (\text{Temp}_t, \text{Temp}_{t-1}, \dots, \text{Temp}_{t-(E-1)}) \in \mathbb{R}^{E} $$

and targets

$$ y_t = \text{Crime}_{t} .$$

::: {.callout-note}
The $\mathbf{x}_t$'s are called _points_ (on the shadow manifold).
:::

<!-- 
## Key idea {data-visibility="uncounted"}

Use lags of the time series!

Given $x_t$ time series, create $E$-length trajectories

$$ \mathbf{x}_t = (x_t, x_{t-\tau}, x_{t-2\tau}, \dots, x_{t-(E-1)\tau}) \in \mathbb{R}^{E} , t=1,2,\dots$$

and targets

$$ y_t = (x_{t+p}) , t=1,2,\dots .$$

Parametrised by $\tau, p \in \mathbb{N}$.

-->

## Split the data

- $\mathcal{L} = \{ (\mathbf{x}_1, y_1) , \dots , (\mathbf{x}_{n} , y_{n}) \}$
is _library set_,
- $\mathcal{P} = \{ (\mathbf{x}_{n+1}, y_{n+1}) , \dots , (\mathbf{x}_{T}, y_{T}) \}$
is _prediction set_.

<br>

For point $\mathbf{x}_{s} \in \mathcal{P}$, pretend we don't know $y_s$ and try to predict it.

$$ \forall \, \mathbf{x} \in \mathcal{L} \quad \text{ find } \quad d(\mathbf{x}_{s}, \mathbf{x}) $$

This is computationally demanding.

<!--

## Non-parametric prediction: simplex

<br>

For point $\mathbf{x}_{s} \in \mathcal{P}$, find $k$ nearest neighbours in $\mathcal{L}$.

Say, e.g., $k=2$ and the neighbours are 

$$\mathcal{NN}_k = \bigl( (\mathbf{x}_{3}, y_3), (\mathbf{x}_{5}, y_5) \bigr)$$

The _simplex method_ predicts

$$\widehat{y}_s = w_1 y_3 + w_2 y_5 .$$ 

## Non-parametric prediction: S-map

<br>

_Sequential Locally Weighted Global Linear Maps (S-map)_

Weight the points by distance
$$ w_i = \exp\bigl\{ - \theta \, d(\mathbf{x}_{s}, \mathbf{x}_i) \bigr\} .$$

Build a local linear system
$$\widehat{y}_s = \mathbf{x}_s^\top \boldsymbol{\beta}_s  .$$

For all $s \in \mathcal{P}$, compare $\widehat{y}_s$ to true $y_s$, and calculate $\rho$.

-->

## Convergent cross mapping

<br>

- If $\text{Temp}_t$ causes $\text{Crime}_t$, then information about $\text{Temp}_t$ is somehow embedded in $\text{Crime}_t$.

- By observing $\text{Crime}_t$, we should be able to forecast $\text{Temp}_t$.

- By observing more of $\text{Crime}_t$ (more "training data"), our forecasts of $\text{Temp}_t$ should be more accurate.

<br>

_Example_: [Chicago crime and temperature](https://edm-developers.github.io/fastEDM-r/articles/chicago.html).


# Software {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Stata package

<iframe src="https://edm-developers.github.io/edm-stata/" width="100%" height="600" style="border:none;"></iframe>

## R package {.smaller}

<br>

Thanks to Rishi Dhushiyandan for his hard work on `easy_edm`.

<br>

<iframe src="https://edm-developers.github.io/fastEDM-r/" width="100%" height="480" style="border:none;"></iframe>

::: footer
:::

## Python package

<iframe src="https://edm-developers.github.io/fastEDM-python/" width="100%" height="600" style="border:none;"></iframe>

## Modern engineering

- Open code (9,745 LOC) on MIT License,
- unit & integration tests (5,342 LOC),
- documentation (5,042 LOC),
- Git (1,198 commits),
- Github Actions (11 tasks),
- vectorised, microbenchmarking, ASAN, linting, 
- all C++ compilers, WASM, all OSs.

<!-- 

wc -l edm-stata/src/*.h edm-stata/src/*.cpp edm-stata/src/main/benchmark.cpp edm-stata/src/main/cli_main.cpp edm-stata/src/main/stata.cpp edm-stata/stata/edm.ado fastEDM-r/R/* fastEDM-python/fastEDM/*.py

wc -l edm-stata/src/main/test.cpp edm-stata/test/ci-test.do fastEDM-r/tests/testthat/* fastEDM-python/tests/*.py

wc -l Stata\ Paper/sj-edm/edm.tex edm-stata/stata/edm.sthlp edm-stata/docs/*.md edm-stata/docs/examples/chicago.md edm-stata/docs/examples/logistic-map.md edm-stata/docs/assets/*js fastEDM-r/man/*.Rd fastEDM-r/man/chicago-easy-edm-example.R fastEDM-r/vignettes/chicago.Rmd fastEDM-python/docs/*.md fastEDM-python/docs/manifolds/manifolds.py

git rev-list --all --count
-->

#

<h2>Get involved!</h2>

<br>

üòä Give it a try, feedback would be very welcome.

<br>

üòç If you're talented in causal inference or programming (Stata/Mata, R, Javascript, C++, Python), we'd love contributions! 

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
