---
title: Empirical Dynamic Modelling
subtitle: Automatic Causal Inference and Forecasting
institute: UNSW Sydney, School of Risk and Actuarial Studies
author: Patrick Laub
date: 7 February 2023
date-format: long
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    slide-number: c/t
    strip-comments: true
    margin: 0.2
    chalkboard:
      boardmarker-width: 5
      grid: false
    include-before: <div class="line right"></div>
    include-after: <script>registerRevealCallbacks();</script>
    code-line-numbers: false
    # footer: Patrick Laub, Stochastic Calculus Seminar, Université de Strasbourg
highlight-style: breeze
jupyter: python3
execute:
  keep-ipynb: true
  echo: true
---

```{python}
#| echo: false
import numpy as np
import numpy.random as rnd
import matplotlib.pyplot as plt
import pandas as pd
from scipy import stats
from bs4 import BeautifulSoup
from IPython.display import display, HTML

import matplotlib

import cycler
colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  plt.rcParams['figure.figsize'] = (2.0, 2.0)

def set_rectangular_figures():
  plt.rcParams['figure.figsize'] = (5.0, 2.0)

set_rectangular_figures()

plt.rcParams['figure.dpi'] = 350
plt.rcParams['savefig.bbox'] = "tight"
plt.rcParams['font.family'] = "serif"

plt.rcParams['axes.spines.right'] = False
plt.rcParams['axes.spines.top'] = False

np.set_printoptions(precision=2, threshold=100)

import re

def trim_html_table(html_string, start=3, end=2):
  soup = BeautifulSoup(html_string, 'html.parser')

  # find the row containing "..."
  dots_row = soup.find("td", string="...")

  if dots_row is None:
    return html_string

  target_row = soup.find("td", string="...").parent

  if start:
    # remove all rows after the target row
    for row in list(reversed(target_row.find_previous_siblings("tr")))[start:]:
        row.decompose()

  if end:
    # remove all rows after the target row
    for row in target_row.find_next_siblings("tr")[:-end]:
        row.decompose()

  # the modified html string
  modified_html = str(soup)

  # Find the substring in the format "N rows x M columns" and convert it to "(N, M)"
  regex = re.compile(r"\d+ rows × \d+ columns")
  match = regex.search(modified_html)
  if match:
    modified_html = modified_html.replace(match.group(), "") # f"({match.group()})")

  return modified_html


def wrapped_html_repr(fn):
    def wrapped(*args, **kwargs):
        x = fn(*args, **kwargs)
        return trim_html_table(x, end=2)
    return wrapped

pd.DataFrame._repr_html_ = wrapped_html_repr(pd.DataFrame._repr_html_)

# Create a `display_vector` function that calls `trim_html_table` on the output of `_repr_html_`
# but also discards the first column of the generated table.
def display_vector(x, start=None, end=2):
    table = trim_html_table(x._repr_html_(), start=start, end=end)
    
    # Use BeautifulSoup to remove the first column of the table.
    # That is, remove the first <th> inside <thead>, and every <th> inside <tbody>.
    soup = BeautifulSoup(table, 'html.parser')
    soup.find("thead").find("th").decompose()
    for row in soup.find("tbody").find_all("th"):
        row.decompose()
    table = str(soup)
    display(HTML(table))
```

# 

<h2>About Me & Outline</h2>


::: columns
::: {.column width="65%"}

<br>

- Software engineer & maths (UQ)
- PhD in probability (Aarhus)
- Post-doc (ISFA Lyon)
- RSE (Uni Melbourne)
- Lecturer (UNSW)

:::
::: {.column width="35%"}

<br>

1. Brief ABC game
2. Empirical Dynamic Modelling
3. [Time permitting] Performant code

:::
:::

<br>
<br>

::: footer
Cf. [https://pat-laub.github.io](https://pat-laub.github.io).
:::

# ABC {data-visibility="uncounted" background-image="unsw-yellow-shape.png"}

![Our IME paper](abc-title.png)

## What is the bias?

We flip a coin three times and get:

::: columns
::: {.column width="33%"}

![**Heads**](aud-heads.jpeg)

:::
::: {.column width="33%"}

![**Tails**](aud-tails.jpeg)
:::
::: {.column width="33%"}

![**Heads**](aud-heads.jpeg)

:::
:::

What is $\theta = \mathbb{P}(\text{Heads})$?

## Let's flip coins

<center>

<img src="qr-code.png" height="400px"/>

[https://pat-laub.github.io/abc-party/game](https://pat-laub.github.io/abc-party/game)

</center>

{{< include _coin-flip-animations.qmd >}}

# Causal Inference Introduction {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Goal: automatic causal inference

<br>

``` r
df <- read.csv("chicago.csv")
head(df)
#>   Time Temperature Crime
#> 1    1       24.08  1605
#> 2    2       19.04  1119
#> 3    3       28.04  1127
#> 4    4       30.02  1154
#> 5    5       35.96  1251
#> 6    6       33.08  1276

library(fastEDM)

crimeCCMCausesTemp <- easy_edm("Crime", "Temperature", data=df)
#> ✖ No evidence of CCM causation from Crime to Temperature found.

tempCCMCausesCrime <- easy_edm("Temperature", "Crime", data=df)
#> ✔ Some evidence of CCM causation from Temperature to Crime found.
```

## {.smaller}

<div style="border: 0px dashed #555"><img src="stata-journal-paper-title.png" /></div>

::: columns
::: column
Jinjing Li<br>
University of Canberra

George Sugihara<br>
University of California San Diego
:::
::: column
Michael J. Zyphur<br>
University of Queensland

Patrick J. Laub<br>
UNSW
:::
:::

<br>

::: callout-note
##  Acknowledgments

Discovery Project DP200100219 and Future Fellowship FT140100629.
:::

## Mirage correlation

::: {.r-stack}
![](mirage-correlations.png)

![](mirage-correlations-sugihara-2012-figure-1.png){.fragment}
:::

::: footer
Source: Sugihara et al. (2012), Detecting causality in complex ecosystems.
:::

## A different view of causality

<br>

Imagine $x_t$, $y_t$, $z_t$ are interesting time series...

_If_ the data is generated according to the nonlinear system:

$$
\begin{aligned}
  x_{t+1}  &= \sigma (y_t - x_t) \\
  y_{t+1}  &= x_t (\rho - z_t) - y_t \\
  z_{t+1}  &= x_t y_t - \beta z_t
\end{aligned}
$$

then $y \Rightarrow x$, both $x, z \Rightarrow y$, and both $x, y \Rightarrow z$.

## Fish brains {.smaller}

<center>
<iframe width="800" height="500" src="https://www.youtube.com/embed/5HtXYKKRA8g?start=1339" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
</center>

Gerald Pao (2021), "How to use Causality without Correlation to Download Brains into Computers", QMNET Seminar.


## Linear/nonlinear dynamical systems {data-visibility="uncounted"}

<br>

Say $\mathbf{x}_t = (x_t, y_t, z_t)$, then if:

::: columns
::: column

$$ \mathbf{x}_{t+1} = \mathbf{A} \mathbf{x}_{t} $$

we have a linear system.

:::
::: column

$$ \mathbf{x}_{t+1} = f(\mathbf{x}_{t}) $$

we have a nonlinear system.

:::
:::

> Using a term like nonlinear science is like referring to the bulk of zoology as the study of non-elephant animals. (Stanisław Ulam)

::: footer
We don't fit a model for $f$, non-parametrically use the data.
Hence the name _empirical_ dynamic modelling. 
:::

## Unobserved variables {data-visibility="uncounted"}

Takens' theorem to the rescue, though...

> Takens' theorem is a deep mathematical result with far-reaching implications.
> Unfortunately, to really understand it, it requires a background in topology.
> (Munch et al. 2020)

::: columns
::: column
![](takens-header.png)
![](takens-contents.png)
:::
::: column
![](takens-simplified.png)
:::
:::

::: footer
Source: Munch et al. (2020), Frequently asked questions about nonlinear dynamics and empirical dynamic modelling, ICES Journal of Marine Science.
:::

# Simplex Algorithm {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Toy problem

```{python}
#| echo: false
rnd.seed(42)

f = 0.137

a = 2 * np.cos(2*np.pi*f)
b = -1

N = 100
t = np.arange(N).astype(float)
x = np.zeros(N)
x[1] = 1.0

for i in range(2, N):
    x[i] = a * x[i-1] + b * x[i-2] + rnd.normal(0, 0.1)

x = x.round(2)
```

::: columns
::: {.column width=75%}

<!-- 
$$
x_t = a x_{t-1} + b x_{t-2} \leftrightarrow x_t = \sin(2 \pi f t)
$$
-->

<br>

```{python}
#| echo: false
plt.rcParams['figure.figsize'] = (5.0, 3.0)
 
# Plot the x vector, adding dots for each point and connecting them with lines
plt.plot(t, x, 'o', color=colors[0])
plt.plot(t, x, '--', color=colors[0], alpha=0.5)
plt.xlabel("$t$")
plt.ylabel("$x$");

set_rectangular_figures()
```

:::
::: {.column width=25%}

```{python}
#| echo: false
df = pd.DataFrame({"\(x_t\)": x})
df.columns.name = "\(t\)"
df
```

:::
:::

<!-- ## Start with time series -->

::: columns
::: {.column width=40%}

```{python}
#| echo: false
# df = pd.DataFrame({"x": x})
# df.columns.name = "t"
# df
```

:::
::: {.column width=40%}

```{python}
#| echo: false
# target = df.copy()
# target.columns.name = "t"
# target.columns = ["next"]
# target["next"] = target["next"].shift(-1) 
# target
```

:::
:::

## Create the embeddings

```{python}
#| echo: false
import fastEDM

E = 3
p = 1
M, y = fastEDM.create_manifold_and_targets(t, x, E=E, tau=1, p=p)

# M = M[:, ::-1]  # reverse the order of the columns

manifold = pd.DataFrame(M)
manifold.columns = ["\(x_t\)"] + ["\(x_{{t-{}}}\)".format(i) for i in range(1, E)]
manifold.columns.name = "\(i\)"

targets = pd.DataFrame(y)
targets.columns = ["\(x_{t+1}\)"]
targets.columns.name = "\(i\)"
```

::: columns
::: {.column width=25%}

```{python}
#| echo: false
display(df)
```

:::
::: {.column width=50%} 

```{python}
#| echo: false
display(manifold) 
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display(targets) 
```

:::
:::

## Making a prediction

```{python}
#| echo: false
point = manifold.iloc[-1, :].values
target = targets.iloc[-1, :].values 

point = [0.57, 0.42, 0.11]
target = -0.25
```

```{python}
#| echo: false
HTML(f"\[ \\boldsymbol{{x}}^* = \\bigl({', '.join(map(str, reversed(point)))} \\bigr) \]")
```

```{python}
#| echo: false
plt.plot(point, 'o', color=colors[0])
plt.plot(point, '--', color=colors[0], alpha=0.5)

plt.axvline(len(point), color=colors[1], ls="--")

# ticks = ["$x_{{t{}}}$".format(i) for i in range(-(E-1), 0)] + ["$x_t$", f"$x_{{t+{p}}}$"]
ticks = ["$t{}$".format(i) for i in range(-(E-1), 0)] + ["$t$", f"$t+{p}$"]

# Set the x tick labels to be the `ticks` variable
plt.xticks(range(len(ticks)), ticks)
plt.ylabel("$x$")
None
```

## Look at the manifold

```{python}
#| echo: false
for i in range(manifold.shape[0]-1): 
  point_i = manifold.iloc[i, :].values
  target_i = targets.iloc[i, 0] 
  plt.plot(point_i, 'o-')
  
  # Plot another line from point_i[-1] to target_i using the same colour as the previous line
  prev_col = plt.gca().lines[-1].get_color()
  
  xs = [len(point_i)-1, len(point_i)]
  ys = [point_i[-1], target_i]

  plt.plot(xs, ys, '--', color=prev_col, alpha=0.5)

plt.axvline(len(point), ls="--", color=colors[1])
plt.xticks(range(len(ticks)), ticks)
plt.ylabel("$x$")
None
```

## Calculate the distances to the point

```{python}
#| echo: false
distances = np.linalg.norm(manifold - point, axis=1)
distances = distances.round(2)
distances = pd.DataFrame({"\(d(\mathbf{x}_i, \mathbf{x}^*)\)": distances})
distances.columns.name = "\(i\)"
``` 

::: columns
::: {.column width=45%} 

```{python}
#| echo: false
display(manifold)
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display_vector(targets)
```

:::
::: {.column width=30%}

```{python}
#| echo: false
display_vector(distances)
```

:::
:::


## Find the nearest neighbours

```{python}
#| echo: false
# Sort the manifold, targets and distances by distance 
order = np.argsort(distances.to_numpy().flatten())
mani_sorted = manifold.iloc[order,:]
targets_sorted = targets.iloc[order]
distances_sorted = distances.iloc[order]
```

::: columns
::: {.column width=45%} 

```{python}
#| echo: false
display(mani_sorted) 
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display_vector(targets_sorted) 
```

:::
::: {.column width=30%}

```{python}
#| echo: false
display_vector(distances_sorted)
```

:::
:::

## Find the nearest neighbours {data-visibility="uncounted"}


```{python}
#| echo: false
# Sort the manifold, targets and distances by distance 
k = len(point) + 1
mani_knn = mani_sorted.head(k)
targets_knn = targets_sorted.head(k)
distances_knn = distances_sorted.head(k)
```


::: columns
::: {.column width=45%} 

```{python}
#| echo: false
display(mani_knn) 
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display_vector(targets_knn) 
```

:::
::: {.column width=30%}

```{python}
#| echo: false
display_vector(distances_knn)
```

:::
:::

## Plot those trajectories

```{python}
#| echo: false
for i in range(mani_knn.shape[0]): 
  point_i = mani_knn.iloc[i, :].values
  target_i = targets_knn.iloc[i, 0] 
  plt.plot(point_i, 'o-')
  
  # Plot another line from point_i[-1] to target_i using the same colour as the previous line
  prev_col = plt.gca().lines[-1].get_color()
  
  xs = [len(point_i)-1, len(point_i)]
  ys = [point_i[-1], target_i]

  plt.plot(xs, ys, '--', color=prev_col, alpha=0.5)

plt.axvline(len(point), ls="--", color=colors[1])
plt.xticks(range(len(ticks)), ticks)
plt.ylabel("$x$")
None
```

## Make a prediction

```{python}
#| echo: false
theta = 1
weights_knn = np.exp(-theta * distances_knn / distances_knn.iloc[0, 0])
weights_knn = weights_knn / weights_knn.sum()
weights_knn = weights_knn.round(2)
weights_knn.columns = ["\(w_i\)"]
weights_knn.columns.name = "\(i\)"
```

::: columns

::: {.column width=25%} 

```{python}
#| echo: false
display(targets_knn) 
```

:::
::: {.column width=25%}

```{python}
#| echo: false
display_vector(distances_knn)
```

:::
::: {.column width=25%}

```{python}
#| echo: false
norm_distances_knn = distances_knn / distances_knn.iloc[0, 0]
norm_distances_knn = norm_distances_knn.round(2)
norm_distances_knn.columns = ["\(\tilde{d}(\mathbf{x}_i, \mathbf{x}^*)\)"]
display_vector(norm_distances_knn)
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display_vector(weights_knn) 
```

:::
:::

<br>
$$w_i = \frac{
  \exp\bigl\{ -\theta \, \tilde{d}(\mathbf{x}_i, \mathbf{x}^*) \bigr\}
  }{
    \sum_{j=1}^k \exp\bigl\{ -\theta \, \tilde{d}(\mathbf{x}_j, \mathbf{x}^*) \bigr\}
  }
$$

## Plot those trajectories (weighted)

```{python}
#| echo: false
for i in range(mani_knn.shape[0]): 
  point_i = mani_knn.iloc[i, :].values
  target_i = targets_knn.iloc[i, 0]
  weight_i = weights_knn.iloc[i, 0]
  plt.plot(point_i, 'o-', alpha=weight_i)
  
  # Plot another line from point_i[-1] to target_i using the same colour as the previous line
  prev_col = plt.gca().lines[-1].get_color()
  
  xs = [len(point_i)-1, len(point_i)]
  ys = [point_i[-1], target_i]

  plt.plot(xs, ys, '--', color=prev_col, alpha=0.75 * weight_i)

plt.axvline(len(point), ls="--", color=colors[1])
plt.xticks(range(len(ticks)), ticks)
plt.ylabel("$x$")
None
```

# S-map Algorithm {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

<br>
<center>

_Sequential Locally Weighted Global Linear Maps_

</center>

## Lorenz system 

Given $x_t$ from the Lorenz system:

```{python}
#| echo: false
df = pd.read_csv("lorenz-rho-50-time-20.csv")
df.columns = ["\(x_t\)", "\(y_t\)", "\(z_t\)"]
df.columns.name = "\(t\)"

df["\(x_t\)"] = df["\(x_t\)"].round(2)
x = df["\(x_t\)"].to_numpy(dtype=float)
x = x.round(2)
t = np.arange(len(x)).astype(float)
```

```{python}
#| echo: false
df["\(x_t\)"].plot(ylabel="$x_t$", xlabel="$t$");
```

predict $x_{t+10}$ given $(x_t, x_{t-20})$.

## Time series to embedding

```{python}
#| echo: false
E = 2
theta = 5.0
tau = 20
p = 10

T = int(np.round(len(t) * 0.9))
M, y = fastEDM.create_manifold_and_targets(t[:T], x[:T], E=E, tau=tau, p=p)

M_pred, y_pred = fastEDM.create_manifold_and_targets(t[T:], x[T:], E=E, tau=tau, p=p)
point = M_pred[-1,:].copy()
true_target = y_pred[-1,0]

manifold = pd.DataFrame(M)
manifold.columns = ["\(x_t\)"] + ["\(x_{{t-{}}}\)".format(i*20) for i in range(1, E)]
manifold.columns.name = "\(i\)"

targets = pd.DataFrame(y)
targets.columns = ["\(x_{t+10}\)"]
targets.columns.name = "\(i\)"
```


::: columns
::: {.column width=25%}

```{python}
#| echo: false
display(df)
```

:::
::: {.column width=50%} 

```{python}
#| echo: false
display(manifold) 
```

:::
::: {.column width=25%} 

```{python}
#| echo: false
display(targets) 
```

:::
:::

## Calculate normalised distances

```{python}
#| echo: false
distances = np.linalg.norm(manifold - point, axis=1)
distances = pd.DataFrame({"\(d(\mathbf{x}_i, \mathbf{x}^*)\)": distances})
distances.columns.name = "\(i\)"
norm_distances = distances / distances.mean()
distances = distances.round(2)
norm_distances = norm_distances.round(2)
norm_distances.columns = ["\(\hat{d}(\mathbf{x}_i, \mathbf{x}^*)\)"]

weights = np.exp(-theta * norm_distances)
weights = weights.round(2)
weights.columns = ["\(w_i\)"]
weights.columns.name = "\(i\)"
w = weights.to_numpy().flatten()

# Add a column of ones to the left of the manifold matrix
manifold_with_const = np.hstack([np.ones((manifold.shape[0], 1)), manifold.to_numpy()])

A = w[:,None] * manifold_with_const
B = (w * targets.to_numpy().flatten())
C_t = np.linalg.lstsq(A, B, rcond=None)[0] 
``` 

::: columns
::: {.column width=40%} 

```{python}
#| echo: false
display(manifold) 
```

:::
::: {.column width=20%} 

```{python}
#| echo: false
display_vector(targets) 
```

:::
::: {.column width=20%}

```{python}
#| echo: false
display_vector(distances)
```

:::
::: {.column width=20%}

```{python}
#| echo: false
display_vector(norm_distances)
```

:::
:::

The average of the distances is:
```{python}
#|echo: false
np.round(np.mean(np.array(distances)), 2)
```

## Weights {data-visibility="uncounted"}

::: columns
::: {.column width=40%} 

```{python}
#| echo: false
display(manifold) 
```

:::
::: {.column width=30%} 

```{python}
#| echo: false
display_vector(distances) 
```

:::
::: {.column width=30%}

```{python}
#| echo: false
display_vector(weights)
```

:::
:::

$$w_i = \exp\bigl\{ -\theta \, \hat{d}(\mathbf{x}_i, \mathbf{x}^*) \bigr\}$$

```{python}
#| echo: false
from sklearn import linear_model

X = manifold.to_numpy()
Y = targets.to_numpy().flatten()

ols = linear_model.LinearRegression()
unweighted_model = ols.fit(X, Y)
```

```{python}
#| echo: false
ols = linear_model.LinearRegression()
weighted_model = ols.fit(X, Y, sample_weight=weights.to_numpy().flatten())
```

## Linear auto-regression 

```{python}
#| echo: false
fig, axes = plt.subplots(1, E, figsize=(8, 3))

for i, ax in enumerate(axes):
  ax.scatter(manifold.iloc[:, i], targets, s=0.5)

  ax.set_xlabel("$x_{t}$")
  if i > 0:
    ax.set_xlabel(f"$x_{{t-{tau*i}}}$")

  xs = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)
  preds = np.zeros(len(xs))

  for j, x_j in enumerate(xs):
    avg_point = np.zeros(E)
    avg_point[0:] = manifold.mean(axis=0).values
    avg_point[i] = x_j
    preds[j] = unweighted_model.predict(avg_point[np.newaxis,:])

  ax.plot(xs, preds, color=colors[1], ls="--", alpha=0.5)

  ax.scatter([point[i]], [true_target], marker="x")

axes[0].set_ylabel(f"$x_{{t+{p}}}$")

plt.tight_layout()
None
```

## Linear regression plane

::: columns
::: column

```{python}
#| echo: false
# Create range for each dimension
xx_pred = np.linspace(X[:,0].min(), X[:,0].max(), 30)
yy_pred = np.linspace(X[:,1].min(), X[:,1].max(), 30)
xx_pred, yy_pred = np.meshgrid(xx_pred, yy_pred)
z = Y

model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

predicted = unweighted_model.predict(model_viz)

# Plot model visualization
fig = plt.figure(figsize=(4, 4))
ax = plt.axes(projection='3d')

ax.plot(X[:,0], X[:,1], Y, zorder=15, linestyle='none', marker='.', alpha=0.125)
ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, s=15, facecolor=(0,0,0,0), edgecolor=colors[1])

ax.set_xlabel("$x_{t}$")
ax.set_ylabel("$x_{t-20}$")
ax.set_zlabel("$x_{t+10}$")

ax.view_init(elev=27, azim=-60)

fig.tight_layout()
```

:::
::: column

::: fragment

```{python}
#| echo: false
# Create range for each dimension
xx_pred = np.linspace(X[:,0].min(), X[:,0].max(), 30)
yy_pred = np.linspace(X[:,1].min(), X[:,1].max(), 30)
xx_pred, yy_pred = np.meshgrid(xx_pred, yy_pred)

model_viz = np.array([xx_pred.flatten(), yy_pred.flatten()]).T

predicted = weighted_model.predict(model_viz)

# Plot model visualization
fig = plt.figure(figsize=(4, 4))
ax = plt.axes(projection='3d')

ax.plot(X[:,0], X[:,1], Y, zorder=15, linestyle='none', marker='.', alpha=0.125)
ax.scatter(xx_pred.flatten(), yy_pred.flatten(), predicted, s=15, facecolor=(0,0,0,0), edgecolor=colors[1])

ax.set_xlabel("$x_{t}$")
ax.set_ylabel("$x_{t-20}$")
ax.set_zlabel("$x_{t+10}$")

ax.view_init(elev=27, azim=-60)

fig.tight_layout()
```

:::

:::
:::

## Weighted linear regression


```{python}
#| echo: false
fig, axes = plt.subplots(1, E, figsize=(8, 3))

for i, ax in enumerate(axes):
  ax.scatter(manifold.iloc[:, i], targets, alpha=weights, s=0.5)
  ax.set_xlabel("$x_{t}$")
  if i > 0:
    ax.set_xlabel(f"$x_{{t-{tau*i}}}$")

  xs = np.linspace(ax.get_xlim()[0], ax.get_xlim()[1], 100)
  preds = np.zeros(len(xs))

  for j, x_j in enumerate(xs):
    avg_point = np.zeros(E)
    avg_point[0:] = (manifold.values * (weights.values / (weights.values.sum()))).sum(axis=0)
    avg_point[i] = x_j
    preds[j] = weighted_model.predict(avg_point[np.newaxis,:])

  ax.plot(xs, preds, color=colors[1], ls="--", alpha=0.5)

  ax.scatter([point[i]], [true_target], marker="x")
  
axes[0].set_ylabel(f"$x_{{t+{p}}}$")

plt.tight_layout()
None
```

# Empirical Dynamic Modelling (EDM) {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Create lagged embeddings

<br>

Given two time series, create $E$-length trajectories

$$ \mathbf{x}_t = (\text{Temp}_t, \text{Temp}_{t-1}, \dots, \text{Temp}_{t-(E-1)}) \in \mathbb{R}^{E} $$

and targets

$$ y_t = \text{Crime}_{t} .$$

::: {.callout-note}
The $\mathbf{x}_t$'s are called _points_ (on the shadow manifold).
:::

<!-- 
## Key idea {data-visibility="uncounted"}

Use lags of the time series!

Given $x_t$ time series, create $E$-length trajectories

$$ \mathbf{x}_t = (x_t, x_{t-\tau}, x_{t-2\tau}, \dots, x_{t-(E-1)\tau}) \in \mathbb{R}^{E} , t=1,2,\dots$$

and targets

$$ y_t = (x_{t+p}) , t=1,2,\dots .$$

Parametrised by $\tau, p \in \mathbb{N}$.

-->

## Split the data

- $\mathcal{L} = \{ (\mathbf{x}_1, y_1) , \dots , (\mathbf{x}_{n} , y_{n}) \}$
is _library set_,
- $\mathcal{P} = \{ (\mathbf{x}_{n+1}, y_{n+1}) , \dots , (\mathbf{x}_{T}, y_{T}) \}$
is _prediction set_.

<br>

For point $\mathbf{x}_{s} \in \mathcal{P}$, pretend we don't know $y_s$ and try to predict it.

$$ \forall \, \mathbf{x} \in \mathcal{L} \quad \text{ find } \quad d(\mathbf{x}_{s}, \mathbf{x}) $$

This is computationally demanding.

<!--

## Non-parametric prediction: simplex

<br>

For point $\mathbf{x}_{s} \in \mathcal{P}$, find $k$ nearest neighbours in $\mathcal{L}$.

Say, e.g., $k=2$ and the neighbours are 

$$\mathcal{NN}_k = \bigl( (\mathbf{x}_{3}, y_3), (\mathbf{x}_{5}, y_5) \bigr)$$

The _simplex method_ predicts

$$\widehat{y}_s = w_1 y_3 + w_2 y_5 .$$ 

## Non-parametric prediction: S-map

<br>

_Sequential Locally Weighted Global Linear Maps (S-map)_

Weight the points by distance
$$ w_i = \exp\bigl\{ - \theta \, d(\mathbf{x}_{s}, \mathbf{x}_i) \bigr\} .$$

Build a local linear system
$$\widehat{y}_s = \mathbf{x}_s^\top \boldsymbol{\beta}_s  .$$

For all $s \in \mathcal{P}$, compare $\widehat{y}_s$ to true $y_s$, and calculate $\rho$.

-->

## Convergent cross mapping

<br>

- If $\text{Temp}_t$ causes $\text{Crime}_t$, then information about $\text{Temp}_t$ is somehow embedded in $\text{Crime}_t$.

- By observing $\text{Crime}_t$, we should be able to forecast $\text{Temp}_t$.

- By observing more of $\text{Crime}_t$ (more "training data"), our forecasts of $\text{Temp}_t$ should be more accurate.

<br>

_Example_: [Chicago crime and temperature](https://edm-developers.github.io/fastEDM-r/articles/chicago.html).


# Software {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Stata package

<iframe src="https://edm-developers.github.io/edm-stata/" width="100%" height="600" style="border:none;"></iframe>

## R package {.smaller}

<br>

Thanks to Rishi Dhushiyandan for his hard work on `easy_edm`.

<br>

<iframe src="https://edm-developers.github.io/fastEDM-r/" width="100%" height="480" style="border:none;"></iframe>

::: footer
:::

## Python package

<iframe src="https://edm-developers.github.io/fastEDM-python/" width="100%" height="600" style="border:none;"></iframe>

## Modern engineering

- Open code (9,745 LOC) on MIT License,
- unit & integration tests (5,342 LOC),
- documentation (5,042 LOC),
- Git (1,198 commits),
- Github Actions (11 tasks),
- vectorised, microbenchmarking, ASAN, linting, 
- all C++ compilers, WASM, all OSs.

<!-- 

wc -l edm-stata/src/*.h edm-stata/src/*.cpp edm-stata/src/main/benchmark.cpp edm-stata/src/main/cli_main.cpp edm-stata/src/main/stata.cpp edm-stata/stata/edm.ado fastEDM-r/R/* fastEDM-python/fastEDM/*.py

wc -l edm-stata/src/main/test.cpp edm-stata/test/ci-test.do fastEDM-r/tests/testthat/* fastEDM-python/tests/*.py

wc -l Stata\ Paper/sj-edm/edm.tex edm-stata/stata/edm.sthlp edm-stata/docs/*.md edm-stata/docs/examples/chicago.md edm-stata/docs/examples/logistic-map.md edm-stata/docs/assets/*js fastEDM-r/man/*.Rd fastEDM-r/man/chicago-easy-edm-example.R fastEDM-r/vignettes/chicago.Rmd fastEDM-python/docs/*.md fastEDM-python/docs/manifolds/manifolds.py

git rev-list --all --count
-->

<!--
## How to choose the hyperparameters?

## For-loop hell
-->

<!--
##  {.smaller}

<h2>Writing faster code</h2>

$$
\begin{aligned}
            \text{Cost of computer's time saved}
            &= \text{Computer \$/hr} \times \text{Fraction of wasteful code removed} \\
            &\quad \times \text{Number of times the code is called} \times \text{Number of users}
\end{aligned}
$$
  
$$\text{Cost of programmer's time spent optimizing}
            \ll \text{Cost of computer's time saved}
$$
-->

# Profiling {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

> "Premature optimization is the root of all evil."
Donald Knuth

>  "... one of the key things is always measure, never trust your instincts because no matter how many years you've been doing this, and I've been doing this for nearly three decades on and off, you're always wrong."
Matt Godbolt

## Random sums example

::: columns
::: column
$$U_n \sim \mathsf{Exp}(\mu)$$
:::
::: column
$$N_t \sim \mathsf{Poisson}(\lambda)$$
:::
:::

::: columns
::: {.column width="33%"}
$$X_t = \textstyle\sum_{n=1}^{N_t} U_n$$
:::
::: {.column width="33%"}
$$\mathbf{X} = (X_t)_{t = 1, \dots, T}$$
:::
::: {.column width="33%"}
$$\text{Need } R \approx 10^7$$
:::
:::

```{python}
#| echo: false
rnd.seed(1)
``` 

```{python}
def sample_geometric_exponential_sums(T, p, μ):
  result = []
  
  for t in range(T):
    N_t = stats.geom(p).rvs()
    X_t = 0.0
    for n in range(N_t):
      U_n = stats.expon(μ).rvs()
      X_t += U_n
    
    result.append(X_t)
      
  return np.array(result)

sample_geometric_exponential_sums(1_000, 0.5, 20.0)
```



## Line profiler

![Line profiler](line-profiler.png)

## "3D" profiler (snakeviz)

![Snakeviz](snakeviz.png)

## Timing 

```{python}
def sample_geometric_exponential_sums(T, p, μ):
  result = []
  
  for t in range(T):
    N_t = stats.geom(p).rvs()
    X_t = 0.0
    for n in range(N_t):
      U_n = stats.expon(μ).rvs()
      X_t += U_n
    
    result.append(X_t)
      
  return result

rnd.seed(1)
%time rvs = sample_geometric_exponential_sums(1_000, 0.5, 20.0)
```

## Timing 

```{python}
def sample_geometric_exponential_sums_alternative(T, p, μ):
  result = []
  
  for t in range(T):
    N_t = stats.geom.rvs(p)
    X_t = 0.0
    for n in range(N_t):
      U_n = stats.expon.rvs(μ)
      X_t += U_n
    
    result.append(X_t)
      
  return result

rnd.seed(1)
%time rvs = sample_geometric_exponential_sums_alternative(1_000, 0.5, 20.0)
```

. . .

```{python}
#| echo: false
t1 = %timeit -n1 -r1 -q -o sample_geometric_exponential_sums(1_000, 0.5, 20.0)
t2 = %timeit -n1 -r1 -q -o sample_geometric_exponential_sums_alternative(1_000, 0.5, 20.0)
print(f"Speedup: {t1.average / t2.average:.2f}x")
```

. . .

::: columns
::: column

```python
N_t = stats.geom(p).rvs()
```

:::
::: column

```python
N_t = stats.geom.rvs(p)
```

:::
:::

<!-- 
##

```python
import numpy.random as rnd

def sample_geometric_exponential_sums(T, p, μ):
  result = []

  for t in range(T):
    N = rnd.geometric(p)
 
    X_t = 0.0
    for n in range(N):
      U_n = rnd.exponential(μ)
      X_t += U_n

    result.append(X_t)

  return result
```

Looking in the source, see that scipy just wraps numpy

Down from 89 ms to 5.5 ms (another 16x speedup)
-->

<!--

# Vectorised code {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Benefits of vectorised code
            
- Readability; more concise, expressing high-level ideas, e.g.
$$ \sum_{n=0}^{\infty} \frac{f^{(n)}(a)}{n!} (x-a)^n $$
- Replace slow interpreted loops with fast compiled code
- Potentially can use a CPU's vectorised instructions (SIMD)
- Better use of the CPU's cache

## Unvectorised {auto-animate=true}

```python
def sample_geometric_exponential_sums(T, p, μ):
  result = []

  for t in range(T):
    N = rnd.geometric(p)
 
    X_t = 0.0
    for n in range(N):
      X_t += rnd.exponential(μ)

    result.append(X_t)

  return result
```

## Vectorised {auto-animate=true}
    
```python
def sample_geometric_exponential_sums(T, p, μ):
  X = np.zeros(T)
  N = rnd.geometric(p, size=T)

  for t in range(T):
    X[t] = rnd.exponential(μ, size=N[t]).sum()

  return X
```

5.5 ms to 5.48 ms (no change)

## Preallocate random variables {auto-animate=true}

```python
def sample_geometric_exponential_sums(T, p, μ):
  X = np.zeros(T) 

  N = rnd.geometric(p, size=T)
  U = rnd.exponential(μ, size=N.sum())

  i = 0
  for t in range(T):
    X[t] = U[i:i+N[t]].sum()
    i += N[t]
  
  return X
```

5.5 ms to 2.7 ms (2x speedup)

-->

# Compiled code {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Talking to a computer

![Arrival](lin-clark-arrival.png)

::: footer
Source: Lin Clark (2017), [A crash course in JIT compilers](https://hacks.mozilla.org/2017/02/a-crash-course-in-just-in-time-jit-compilers/), Mozilla blog.
:::

## Interpreters & compilers

::: {.r-stack}
![](lin-clark-interpreter.png)

![](lin-clark-compiler.png){.fragment}
:::

::: footer
Source: Lin Clark (2017), [A crash course in JIT compilers](https://hacks.mozilla.org/2017/02/a-crash-course-in-just-in-time-jit-compilers/), Mozilla blog.
:::

<!-- <img style="" data-natural-width="413" data-natural-height="360" data-lazy-loaded="" data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/782452/images/8159839/xkcd-compiling.png"> -->

<!-- <a href="https://xkcd.com/303/" target="_blank">xkcd</a> -->

<!-- 
## Why is Python slow?

::: columns
::: column

```python
x = 1
y = 2
z = x + y

x = "Patrick "
y = "Laub"
z = x + y
```

:::
::: column

```cpp
int x = 1;
int y = 2;
int z = x + y;

std::string x2 = "Patrick ";
std::string y2 = "Laub";
std::string z2 = x2 + y2;
```

:::
:::

<img style="" data-natural-width="2346" data-natural-height="922" data-lazy-loaded="" data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/782452/images/8161425/pasted-from-clipboard.png">

Try Compiler Explorer
<a href="https://godbolt.org/z/dTv78M" target="_blank">https://godbolt.org/z/dTv78M</a>

```c
// x = 1
if (PyDict_SetItem(__pyx_d, __pyx_n_s_x, __pyx_int_1) < 0) {
__PYX_ERR(0, 2, __pyx_L1_error);
}

// y = 2
if (PyDict_SetItem(__pyx_d, __pyx_n_s_y, __pyx_int_2) < 0) {
__PYX_ERR(0, 3, __pyx_L1_error);
}

// z = x + y
__Pyx_GetModuleGlobalName(__pyx_t_1, __pyx_n_s_x);
if (unlikely(!__pyx_t_1)) {
__PYX_ERR(0, 4, __pyx_L1_error);
}
__Pyx_GOTREF(__pyx_t_1);
__Pyx_GetModuleGlobalName(__pyx_t_2, __pyx_n_s_y);
if (unlikely(!__pyx_t_2)) {
__PYX_ERR(0, 4, __pyx_L1_error);
}
__Pyx_GOTREF(__pyx_t_2);
__pyx_t_3 = PyNumber_Add(__pyx_t_1, __pyx_t_2);
if (unlikely(!__pyx_t_3)) {
__PYX_ERR(0, 4, __pyx_L1_error);
}
__Pyx_GOTREF(__pyx_t_3);
__Pyx_DECREF(__pyx_t_1);
__pyx_t_1 = 0;
__Pyx_DECREF(__pyx_t_2);
__pyx_t_2 = 0;
if (PyDict_SetItem(__pyx_d, __pyx_n_s_z, __pyx_t_3) < 0) {
__PYX_ERR(0, 4, __pyx_L1_error);
}
__Pyx_DECREF(__pyx_t_3);
__pyx_t_3 = 0;
```

Python

C++
-->

## Type checking

::: columns
::: {.column width="40%"}

<br>
<br>

```R
function arraySum(arr) {
  sum = 0
  for (i in 1:length(arr)) {
    sum = sum + arr[i]
  }
  return(sum)
}
```

<br>

::: {.smaller}
Compiler Explorer: <a href="https://godbolt.org/z/dTv78M" target="_blank">https://godbolt.org/z/dTv78M</a>
:::

:::
::: {.column width="60%"}

![Type checking](lin-clark-type-checking.png)

:::
:::

::: footer
Source: Lin Clark (2017), [A crash course in JIT compilers](https://hacks.mozilla.org/2017/02/a-crash-course-in-just-in-time-jit-compilers/), Mozilla blog.
:::


## Theoretical limits

**Amdahl’s Law** (or Law of Diminishing Returns)

$P$ is the proportion of a program that can be made parallel
$1 - P$ remains serial.
    
<u>Theoretical</u> maximum speedup of achieved by N processor is

$$ S(N)=1/[(1-P)+P/N] $$

Hence with unlimited processor count

$$ S(\infty) = 1 / (1-P) $$

E.g. $P = 0.9$ then $S(\infty) = 10$.

#

<h2>Get involved!</h2>

<br>

😊 Give it a try, feedback would be very welcome.

<br>

😍 If you're talented in causal inference or programming (Stata/Mata, R, Javascript, C++, Python), we'd love contributions! 


# Appendix {data-visibility="uncounted" background-image="unsw-yellow-shape.png"}


## My computer {data-visibility="uncounted"}

![My work machine (for deep learning experiments).](deep-learning-pc.mov)

## Speeds of computing {data-visibility="uncounted"}

![CPU vs RAM speeds.](cpu-vs-ram-speed.png)

<!-- ## A typical program -->

## Cache system {data-visibility="uncounted"}

![Hierarchy of caches](caches.png)

::: footer
[http://cs.brown.edu/courses/csci1310/2020/assign/labs/lab4.html](http://cs.brown.edu/courses/csci1310/2020/assign/labs/lab4.html)
:::

## Impact on matrix order {data-visibility="uncounted"}

```{python}
n = 10_000
matrix = np.array(range(n*n)).reshape(n, n)
```

```{python}
%%time
col_sums = np.zeros(n)
for j in range(n):
  col_sums += matrix[:, j]
```

```{python}
%%time
row_sums = np.zeros(n)
for i in range(n):
  row_sums += matrix[i, :]
```


<!-- <img style="left: -118px; top: -206px; width: 775px; height: 1109px;" data-natural-width="943" data-natural-height="1350" data-crop-x="0.151934" data-crop-y="0.185328" data-crop-width="0.823204" data-crop-height="0.600386" data-lazy-loaded="" data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/782452/images/8164432/0C1C971C-C1C1-4E17-BB3B-1E2C5CD21D35.png"> -->

# Compiled code {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## What does a machine understand? {.smaller data-visibility="uncounted"}


$$
\underbrace{\text{1100 0111}}_{\Large \text{function}}~\underbrace{\text{01000101 1111 1000 0101 1111 1000 0000}}_{\Large \text{arg1}}~\underbrace{\text{0001 0000 000 0000 0000 0000 0000 0000 0000 0000 0000}}_{\Large \text{arg2}}
$$
  

```x86
movl $0x1, -0x8(%rbp)
```

```cpp
int x = 1;
```

Instruction set architecture (ISA), e.g. x86-64

Intel i5 (2010): add (0101), subtract (1111), ...

Intel i7 (2020): add (0101), subtract (1111), add_vectors (1011), ...

Apple M1 (2020): add (100), &nbsp;<span style="color:rgb(51, 51, 51)">subtract (111), ...
            

Try Compiler Explorer <a href="https://godbolt.org/z/dTv78M" target="_blank">https://godbolt.org/z/dTv78M</a>


## {auto-animate=true data-visibility="uncounted"}

```python
def sample_geometric_exponential_sums(T, p, μ):
  X = np.zeros(T) 

  N = rnd.geometric(p, size=T)
  U = rnd.exponential(μ, size=N.sum())

  i = 0
  for t in range(T):
    X[t] = U[i:i+N[t]].sum()
    i += N[t]
  
  return X
```

Interpreted version

## {auto-animate=true data-visibility="uncounted"}

```python
cpdef sample_geometric_exponential_sums(long T, double p, double mu):
  cdef double[:] X = np.zeros(T) 
  cdef long[:] N = rnd.geometric(p, size=T)
  cdef double[:] U = rnd.exponential(mu, size=np.sum(N))

  cdef int i = 0
  cdef int t
  for t in range(T):
    X[t] = np.sum(U[i:i+N[t]])
    i += N[t]
    return X
```

Compiled by <em>Cython</em> (not recommended)

## {auto-animate=true data-visibility="uncounted"}
    
```python
def sample_geometric_exponential_sums(T, p, μ):
  X = np.zeros(T) 

  N = rnd.geometric(p, size=T)
  U = rnd.exponential(μ, size=N.sum())

  i = 0
  for t in range(T):
    X[t] = U[i:i+N[t]].sum()
    i += N[t]
  
  return X
```

Interpreted version

## {auto-animate=true data-visibility="uncounted"}

```python
from numba import njit

@njit()
def sample_geometric_exponential_sums(T, p, μ):
  X = np.zeros(T) 

  N = rnd.geometric(p, size=T)
  U = rnd.exponential(μ, size=N.sum())

  i = 0
  for t in range(T):
    X[t] = U[i:i+N[t]].sum()
    i += N[t]
  
  return X
```

First run is compiling (500 ms), but after we are
down from 2.7 ms to 164 μs (16x speedup)

Compiled by <em>numba</em>

## {data-visibility="uncounted"}

Original code: 1.7 s

Basic profiling with **snakeviz**: 5.5 ms, 310x speedup

+ Vectorisation/preallocation with **numpy**: 2.7 ms, 630x speedup

+ Compilation with **numba**: 164 μs, 10,000x speedup

And potentially:

+ Parallel over 80 cores: say another 50x improvement,
so overall 50,000x speedup.

# Parallel code {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

<!--

## Computing in parallel
    
```python
serialResult = sum(f(i) for i in range(n+1))

from joblib import Parallel, delayed

with Parallel(n_jobs=10) as parallel:
  parResult = sum(parallel(delayed(f)(i) for i in range(n+1)))
```

In python, use <em>joblib</em>. E.g. to calculate

$$ \sum_{i=0}^{n} f(i) $$
-->

## Threads or processes? {.smaller data-visibility="uncounted"}

Use multiple <u>processes</u> in Python,
probably should use multiple threads elsewhere.

::: columns
::: column

<img style="left: -88px; top: 0px; width: 452px; height: 301px;" data-natural-width="1400" data-natural-height="933" data-crop-x="0.19469" data-crop-y="0" data-crop-width="0.590708" data-crop-height="1" data-lazy-loaded="" data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/782452/images/8164195/dolly.jpg">

:::
::: column

<img data-natural-width="1200" data-natural-height="675" style="left: -138px; top: -52px; width: 752px; height: 423px;" data-crop-x="0.183623" data-crop-y="0.123348" data-crop-width="0.681141" data-crop-height="0.715859" data-lazy-loaded="" data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/782452/images/8164197/the-prestige.jpeg">

:::
:::

Baumann et al. (2019), A fork() in the road
17th Workshop on Hot Topics in Operating Systems
    
## {data-visibility="uncounted"}

<img data-natural-width="1933" data-natural-height="1350" style="" data-lazy-loaded="" data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/782452/images/8164371/CDB86CFB-AAC5-434D-9FCE-05A8ACD84FDF.png">

## {data-visibility="uncounted"}

<img data-natural-width="1933" data-natural-height="1350" style="" data-lazy-loaded="" data-crop-x="0" data-crop-y="0" data-crop-width="1" data-crop-height="1" data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/782452/images/8164372/05BCC1DF-A1BE-4589-BD78-E62AF165BB67.png">


## Bayesian approach {data-visibility="uncounted"}

::: columns
::: column
![Prior](coin-prior.png)
:::
::: column

<iframe width="560" height="280" src="https://www.youtube.com/embed/P6pugawb7SM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

<iframe width="560" height="280" src="https://www.youtube.com/embed/tuXXubOkFuY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

:::
:::



<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
